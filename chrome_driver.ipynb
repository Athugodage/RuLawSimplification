{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9592f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait as Wait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import NoSuchFrameException\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#driver.find_element_by_xpath('//*[@id=\"searchResults\"]/div[2]/span').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "url = 'https://rg.ru/doc-search/'\n",
    "ssilka = []\n",
    "host = 'https://rg.ru/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "#sleep(20)\n",
    "#Wait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"rgb_feed_doc-binding\"]/div/div/div/div/a'))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_more():\n",
    "    try:\n",
    "        sleep(10)\n",
    "        element = Wait(driver, 15).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"searchResults\"]/div[2]/span')))\n",
    "        element.click()\n",
    "        print('success')\n",
    "    except ElementClickInterceptedException:\n",
    "        sleep(5)\n",
    "        action = ActionChains(driver)\n",
    "        action.key_down(Keys.END).perform()\n",
    "        sleep(5)\n",
    "        print('performing keys...')\n",
    "        element = Wait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"searchResults\"]/div[2]/span')))\n",
    "        element.click()\n",
    "    except:\n",
    "        print(\"cannot search more\")\n",
    "        \n",
    "\n",
    "for i in range(5):\n",
    "    action = ActionChains(driver)\n",
    "    action.key_down(Keys.END).perform()\n",
    "    search_more()\n",
    "print('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dba977",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "\n",
    "soup = bs(driver.page_source, 'html.parser')\n",
    "\n",
    "with open('articles_links.txt', 'w') as file:\n",
    "    for article in soup.findAll('div', class_='b-news-inner__list-item-wrapper'):\n",
    "        link = article.find('a', href=True)\n",
    "        wholelink = host+link['href']\n",
    "        file.write(wholelink)\n",
    "        file.write('\\n')\n",
    "        page = link['href']\n",
    "        print(wholelink)\n",
    "        pages.append(page)\n",
    "\n",
    "        \n",
    "with open('pages.txt', 'w') as page:\n",
    "    page.write(str(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbfe6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Checks if Oops window opens\n",
    "def oops_factor():\n",
    "    ops = soup1.find('body')\n",
    "    if ops != None:\n",
    "        ops_text = ops.get_text()\n",
    "        if 'oops' in ops_text.lower():\n",
    "            driver.refresh()\n",
    "            sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df84d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def accident(soup1, driver):\n",
    "    sleep(20)\n",
    "    material = soup1.find('div', class_='b-sticked-material-share b-sticked-material-share__header')\n",
    "    if material != None:\n",
    "        return material.get_text()\n",
    "    else:\n",
    "        material = 'N/A'\n",
    "        return material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_text(driver, soup1):\n",
    "    try:\n",
    "        sleep(3)\n",
    "        material = driver.find_element_by_xpath('//*[@id=\"rgb_material-wrapper_doc\"]/article')\n",
    "        if material is not None: \n",
    "            return material.text\n",
    "        else:\n",
    "            material = 'N/A'\n",
    "            return material\n",
    "    except AttributeError:\n",
    "        sleep(3)\n",
    "        material = soup1.find('article', attrs={'class': 'b-material-wrapper__body'})\n",
    "        if material is None:\n",
    "            return accident(soup1, driver)\n",
    "            # аварийный вариант    \n",
    "        else:\n",
    "            print('Text for', driver.current_url)\n",
    "            return material.get_text()\n",
    "        material = 'N/A'\n",
    "        return material\n",
    "    except:\n",
    "        print('NOT WORKING', driver.current_url)\n",
    "        material = 'N/A'\n",
    "        return material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df837d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_comment(driver):\n",
    "    sleep(5)\n",
    "    try:\n",
    "        comment = driver.find_element_by_xpath('//*[@id=\"rgb_feed_doc-binding\"]/div/div[2]/div/div/a')\n",
    "        if comment is not None:\n",
    "            if 'Комментарии Российской Газеты' in comment.text:\n",
    "                comment.click()\n",
    "                comment_text = driver.find_element_by_xpath('//*[@id=\"articleContainer\"]/div[1]')\n",
    "                sleep(5)\n",
    "                if comment_text is not None:\n",
    "                    return comment_text.text\n",
    "                else:\n",
    "                    comment_text = 'N/A'\n",
    "                    return comment_text\n",
    "            else:\n",
    "                comment_text = 'N/A'\n",
    "                return comment_text\n",
    "        else:\n",
    "            comment_text = 'N/A'\n",
    "            return comment_text\n",
    "    except NoSuchElementException:\n",
    "        comment_text= 'N/A'\n",
    "        return comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37cad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#('a', 'b-link b-link_title')\n",
    "    \n",
    "# Это я намудрил    \n",
    "def appropriate(soup1, driver):\n",
    "    test_num = 0\n",
    "    sleep(15)\n",
    "    comment = driver.find_element_by_xpath('//*[@id=\"rgb_feed_doc-binding\"]/div/div[2]/div/div/a')\n",
    "    if comment is not None:\n",
    "        if 'Комментарии Российской Газеты' in comment.text:\n",
    "            sleep(3)\n",
    "            test_num += 2\n",
    "            material = driver.find_element_by_xpath('//*[@id=\"rgb_material-wrapper_doc\"]/article')\n",
    "            if material is not None:   \n",
    "                test_num += 2\n",
    "            else:\n",
    "                accident(soup1, driver)\n",
    "                if material is not None:\n",
    "                    test_num += 2\n",
    "                else:\n",
    "                    test_num -= 2\n",
    "    else: \n",
    "        test_num -= 2\n",
    "    return test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bcef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_date(soup1):\n",
    "    sleep(3)\n",
    "    date = driver.find_element_by_xpath('//*[@id=\"rgb_material-head_art\"]/div[2]/div[1]/span[1]')\n",
    "    if date is not None:\n",
    "        return date.text\n",
    "    else:\n",
    "        date = 'N/A'\n",
    "        return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ebe3676",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-62b9d16d8c07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'links_for_comments.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomment_links\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pages' is not defined"
     ]
    }
   ],
   "source": [
    "def saving():\n",
    "    with open('mydataset.csv', 'w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=';')\n",
    "        writer.writerow(['Название документа', 'Ссылка', 'Текст', 'Комментарий РГ', 'Дата'])\n",
    "        return writer\n",
    "        \n",
    "\n",
    "def data_making():\n",
    "    # NOT READY!!!!\n",
    "    table.append(\n",
    "        {\n",
    "            'Название документа': title,\n",
    "            'Ссылка': driver.current_url, \n",
    "            'Текст': getting_text(driver, soup1, link),\n",
    "            'Комментарий РГ': getting_comment(soup1, host),\n",
    "            'Дата': document_date(soup1),\n",
    "        }\n",
    "    )\n",
    "    saving().writerow([title, driver.current_url, getting_text(driver, soup1, link), getting_comment(soup1, host), document_date(soup1),])\n",
    "    return table\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Нужно сделать так, чтобы программа проверяла на наличие комментария, если его нет - статью мимо\n",
    "n = 0\n",
    "link_comment = []\n",
    "link_comment_reserve = []\n",
    "table = []\n",
    "with open('links_for_comments.txt', 'w', encoding='utf-8') as comment_links:          \n",
    "    for link in pages:\n",
    "        sleep(2)\n",
    "        try:\n",
    "            driver.get(host+link)\n",
    "        except:\n",
    "            driver.navigate(link)\n",
    "            print('Problem with link for', link)\n",
    "        sleep(10)\n",
    "        \n",
    "            \n",
    "        soup1 = bs(driver.current_url, 'html.parser')\n",
    "        title = driver.title\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        \n",
    "        oops_factor()\n",
    "        \n",
    "        if appropriate(soup1, driver) > 3:\n",
    "            sleep(5)\n",
    "            data_making()\n",
    "            n += 1\n",
    "            print('Successful for ', n)\n",
    "        print(f'Unable to parse {driver.current_url}    Sorry!')    \n",
    "        \n",
    "print('I finished')\n",
    "            \n",
    "# material = [x.get_text() for x in soup1.find('article', attrs={'class': 'b-material-wrapper__body'})]    \n",
    "# zeug = [x.get_text() for x in soup.find_all('div', attrs={'class': 'fm_linkeSpalte'})]\n",
    "# #rgb_material-wrapper_doc > article   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
